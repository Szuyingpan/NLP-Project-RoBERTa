{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8e041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236a57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/model_doc/roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0751c",
   "metadata": {},
   "source": [
    "## 1. Important library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fb5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4999121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/szuyingpan/anaconda3/envs/PythonTest/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef35276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eddbd8a",
   "metadata": {},
   "source": [
    "## 2. Important dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbbbb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                             phrase\n",
      "0         1  Stuning even for the non-gamer: This sound tra...\n",
      "1         1  The best soundtrack ever to anything.: I'm rea...\n",
      "2         1  Amazing!: This soundtrack is my favorite music...\n",
      "3         1  Excellent Soundtrack: I truly like this soundt...\n",
      "4         1  Remember, Pull Your Jaw Off The Floor After He...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Initialize lists to store the separated sentiments and phrases\n",
    "sentiments = []\n",
    "phrases = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('train.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Extract sentiment and phrase using regex\n",
    "        match = re.match(r'(__label__\\d) (.*)', line)\n",
    "        \"label_1 means negative (0); label_2 means positive(1)\"\n",
    "        if match:\n",
    "            sentiments.append(match.group(1).replace('__label__1', '0').replace('__label__2', '1'))\n",
    "            phrases.append(match.group(2))\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "train = pd.DataFrame({\n",
    "    'sentiment': sentiments,\n",
    "    'phrase': phrases\n",
    "})\n",
    "\n",
    "# Check the first few rows to ensure it's loaded correctly\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07194821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae66a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             phrase\n",
       "0         1  Stuning even for the non-gamer: This sound tra...\n",
       "1         1  The best soundtrack ever to anything.: I'm rea...\n",
       "2         1  Amazing!: This soundtrack is my favorite music...\n",
       "3         1  Excellent Soundtrack: I truly like this soundt...\n",
       "4         1  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdeb9afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again 0 stands for negative, 1 means positive\n",
    "train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f49cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3600000</td>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                             phrase\n",
       "count    3600000                                            3600000\n",
       "unique         2                                            3600000\n",
       "top            1  Stuning even for the non-gamer: This sound tra...\n",
       "freq     1800000                                                  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f83b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment                                             phrase\n",
      "0         1  Great CD: My lovely Pat has one of the GREAT v...\n",
      "1         1  One of the best game music soundtracks - for a...\n",
      "2         0  Batteries died within a year ...: I bought thi...\n",
      "3         1  works fine, but Maha Energy is better: Check o...\n",
      "4         1  Great for the non-audiophile: Reviewed quite a...\n"
     ]
    }
   ],
   "source": [
    "# Import test\n",
    "\n",
    "import re\n",
    "\n",
    "# Initialize lists to store the separated sentiments and phrases\n",
    "sentiments = []\n",
    "phrases = []\n",
    "\n",
    "# Open the file and read line by line\n",
    "with open('test.ft.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Extract sentiment and phrase using regex\n",
    "        match = re.match(r'(__label__\\d) (.*)', line)\n",
    "        \"label_1 means negative (0); label_2 means positive(1)\"\n",
    "        if match:\n",
    "            sentiments.append(match.group(1).replace('__label__1', '0').replace('__label__2', '1'))\n",
    "            phrases.append(match.group(2))\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "test = pd.DataFrame({\n",
    "    'sentiment': sentiments,\n",
    "    'phrase': phrases\n",
    "})\n",
    "\n",
    "# Check the first few rows to ensure it's loaded correctly\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e005f7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3395102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Great CD: My lovely Pat has one of the GREAT v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Batteries died within a year ...: I bought thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>works fine, but Maha Energy is better: Check o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the non-audiophile: Reviewed quite a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                             phrase\n",
       "0         1  Great CD: My lovely Pat has one of the GREAT v...\n",
       "1         1  One of the best game music soundtracks - for a...\n",
       "2         0  Batteries died within a year ...: I bought thi...\n",
       "3         1  works fine, but Maha Energy is better: Check o...\n",
       "4         1  Great for the non-audiophile: Reviewed quite a..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db0503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00e849",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b6e6c",
   "metadata": {},
   "source": [
    "I will start with defining few key variables that will be used later during the training/fine tuning stage. Followed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. I will also define the Dataloader that will feed the data in batches to the neural network for suitable training and processing. Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the docs at PyTorch\n",
    "\n",
    "SentimentData Dataset Class\n",
    "\n",
    "This class is defined to accept the Dataframe as input and generate tokenized output that is used by the Roberta model for training.\n",
    "I am using the Roberta tokenizer to tokenize the data in the TITLE column of the dataframe.\n",
    "The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: ids, attention_mask\n",
    "\n",
    "To read further into the tokenizer, refer to this document\n",
    "target is the encoded category on the news headline.\n",
    "The SentimentData class is used to create 2 datasets, for training and for validation.\n",
    "Training Dataset is used to fine tune the model: 80% of the original data\n",
    "Validation Dataset is used to evaluate the performance of the model. The model has not seen this data during training.\n",
    "\n",
    "Dataloader\n",
    "Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "This control is achieved using the parameters such as batch_size and max_len.\n",
    "Training and Validation dataloaders are used in the training and validation part of the flow respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e1d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec4266dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.phrase\n",
    "        self.targets = self.data.sentiment\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d91d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Dataset: (3600000, 2)\n",
      "TEST Dataset: (400000, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = train\n",
    "test_data=test\n",
    "\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ff360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0d612",
   "metadata": {},
   "source": [
    "## 4 Creating the Neural Network for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52565c",
   "metadata": {},
   "source": [
    "\n",
    "Neural Network\n",
    "We will be creating a neural network with the RobertaClass.\n",
    "This network will have the Roberta Language model followed by a dropout and finally a Linear layer to obtain the final outputs.\n",
    "The data will be fed to the Roberta Language model as defined in the dataset.\n",
    "Final layer outputs is what will be compared to the Sentiment category to determine the accuracy of models prediction.\n",
    "We will initiate an instance of the network called model. This instance will be used for training and then to save the final trained model for future inference.\n",
    "Loss Function and Optimizer\n",
    "Loss Function and Optimizer and defined in the next cell.\n",
    "The Loss Function is used the calculate the difference in the output created by the model and the actual output.\n",
    "Optimizer is used to update the weights of the neural network to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edafad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83de94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa84160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fcf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999957bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install zstandard\n",
    "from datasets import load_dataset\n",
    "\n",
    "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
    "#data_files = \"/Users/szuyingpan/Desktop/NLP/CW1/train.ft.txt\"\n",
    "train = load_dataset(\"text\", data_files=data_files, split=\"train\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ba346",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_version_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3a4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e060fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install psutil\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe55f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\n",
    "data_files = \"/Users/szuyingpan/Desktop/NLP/CW1/test.ft.txt\"\n",
    "test = load_dataset(\"text\", data_files=data_files, split=\"train\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da413a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d921e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rss attribute refers to the resident set size, which is the fraction of memory that a process occupies \n",
    "#in RAM. This measurement also includes the memory used by the Python interpreter and the libraries we’ve \n",
    "#loaded, so the actual amount of memory used to load the dataset is a bit smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of train dataset\n",
    "print(f\"Number of files in dataset : {train.dataset_size}\")\n",
    "size_gb = train.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c7fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of test dataset\n",
    "print(f\"Number of files in dataset : {test.dataset_size}\")\n",
    "size_gb = test.dataset_size / (1024**3)\n",
    "print(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(train), batch_size):\n",
    "    _ = train[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(train)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64e49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "code_snippet = \"\"\"batch_size = 1000\n",
    "\n",
    "for idx in range(0, len(test), batch_size):\n",
    "    _ = test[idx:idx + batch_size]\n",
    "\"\"\"\n",
    "\n",
    "time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\n",
    "print(\n",
    "    f\"Iterated over {len(test)} examples (about {size_gb:.1f} GB) in \"\n",
    "    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3b624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over it one example at a time without loading the entire dataset into memory.\n",
    "train_streamed = load_dataset(\n",
    "    \"text\", data_files=data_files, split=\"train\", streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11380f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(iter(train_streamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ce562",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_LINES = 10000\n",
    "\n",
    "data = {}\n",
    "\n",
    "# read the archived file line by line, add add it to a map\n",
    "for i, line in enumerate(\"train_streamed\", \"rt\", encoding=\"utf8\")):\n",
    "\n",
    "    if i == NUMBER_OF_LINES:\n",
    "        break\n",
    "\n",
    "    # label 1 is negativ and label 2 is positive\n",
    "    label = 1 if line[:10] == \"__label__1\" else 2\n",
    "    text = line[10:]\n",
    "\n",
    "    localResult = {\n",
    "        \"label\": label,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "    data[i] = localResult\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data).T\n",
    "df = df.reset_index().rename(columns= {\"index\": \"Id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf852da",
   "metadata": {},
   "source": [
    "To work as sentiment analysis, we need to preprocess the text to extract the label and review text separately and then apply further preprocessing like tokenization or stop words removal as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_streamed_data(example):\n",
    "    # Extract label (assuming label 1 is negative and label 2 is positive)\n",
    "    label = 1 if example['text'].startswith(\"__label__1\") else 2\n",
    "    # Remove the label from the text and any leading/trailing whitespace\n",
    "    text = example['text'][10:].strip()\n",
    "    return {\"label\": label, \"text\": text}\n",
    "\n",
    "# Use a generator expression to apply preprocessing\n",
    "preprocessed_stream = (preprocess_streamed_data(example) for example in train_streamed)\n",
    "\n",
    "# Example: Access the first preprocessed example\n",
    "print(next(preprocessed_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c99ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Assuming NLTK data has been downloaded\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.remove('not')  # Keep \"not\" for sentiment analysis\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, HTML tags, and punctuation from text.\"\"\"\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    return text\n",
    "\n",
    "def remove_stopwords_and_punct(text):\n",
    "    \"\"\"Tokenize text and remove stopwords and punctuation, return as a single string.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def preprocess_streamed_data(example):\n",
    "    label = 1 if example['text'].startswith(\"__label__1\") else 2\n",
    "    text = example['text'][10:].strip()\n",
    "    cleaned_text = clean_text(text)\n",
    "    preprocessed_text = remove_stopwords_and_punct(cleaned_text)\n",
    "    return {\"label\": label, \"text\": preprocessed_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RoBERTa model\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_length=512):\n",
    "        self.examples = list(examples)  # Convert generator to list to access its length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]['text']\n",
    "        label = self.examples[idx]['label']\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeddd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming preprocessed_stream is your preprocessed data\n",
    "preprocessed_list = [preprocess_streamed_data(example) for example in train_streamed]  # Convert generator to list\n",
    "dataset = SentimentDataset(preprocessed_list, tokenizer)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Function\n",
    "\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm  # for displaying progress bar\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()  # set model to training mode\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f\"Training loss: {average_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c14423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing for training\n",
    "# Initialize the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # move model to the right device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract negative sentiment is encoded as 0 and positive sentiment as 1\n",
    "\n",
    "def preprocess_streamed_data(example):\n",
    "    # Adjust labels to be in the range [0, 1]\n",
    "    label = 0 if example['text'].startswith(\"__label__1\") else 1  # Adjusted labels here\n",
    "    text = example['text'][10:].strip()\n",
    "    return {\"label\": label, \"text\": text}\n",
    "\n",
    "# Then, you create the preprocessed list again with the adjusted labels\n",
    "preprocessed_list = [preprocess_streamed_data(example) for example in train_streamed]\n",
    "\n",
    "\n",
    "dataset = SentimentDataset(preprocessed_list, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556dd2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example code to set device in PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    epochs = 3  # example for 3 epochs, adjust as needed\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        train(model, train_loader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3  # example for 3 epochs, adjust as needed\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_loader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a41179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "    \n",
    "    # Convert outputs to predictions\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8884f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume val_loader is already created and similar to train_loader but for validation data\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train(model, train_loader, optimizer, device)  # Training step\n",
    "    metrics = evaluate(model, val_loader, device)  # Evaluation step\n",
    "    print(f\"Validation Accuracy: {metrics['accuracy']}\")\n",
    "    print(f\"Validation F1 Score: {metrics['f1']}\")\n",
    "    print(f\"Validation Precision: {metrics['precision']}\")\n",
    "    print(f\"Validation Recall: {metrics['recall']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc13bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec29bd67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf203bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0704d128",
   "metadata": {},
   "source": [
    "Loading a pre-trained RoBERTa model and its tokenizer, preparing your data in the format expected by the model, and then either fine-tuning the model on your dataset or using the model to make predictions directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938c836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "#from torch.utils.data import DataLoader, Dataset\n",
    "#import torch\n",
    "# Load the RoBERTa tokenizer and model\n",
    "#tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e7d6f",
   "metadata": {},
   "source": [
    "Since RoBERTa expects raw text as input for its tokenizer to work correctly (because it handles special tokens and segmentation itself), you'll need to convert your tokens back into text strings before using the RoBERTa tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "#class TokenizedReviewsDataset(Dataset):\n",
    " #   def __init__(self, tokenized_reviews, labels, tokenizer, max_length=512):\n",
    "        self.tokenized_reviews = tokenized_reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert list of tokens back to string\n",
    "        review_text = \" \".join(self.tokenized_reviews[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Encode the review text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review_text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',  # PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60618eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the stream to lists for training\n",
    "tokenized_reviews = []\n",
    "labels = []\n",
    "\n",
    "for example in preprocessed_text_stream:\n",
    "    # Join tokens to form a single string (if your dataset preparation requires strings)\n",
    "    review_text = \" \".join(example['tokens'])\n",
    "    tokenized_reviews.append(review_text)\n",
    "    labels.append(example['label'])\n",
    "\n",
    "# Now tokenized_reviews and labels are populated and can be used to create the dataset\n",
    "\n",
    "\n",
    "# Create DataLoader for the Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Assuming tokenized_reviews and labels are available\n",
    "dataset_size = len(tokenized_reviews)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "full_dataset = TokenizedReviewsDataset(tokenized_reviews, labels, tokenizer)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from transformers import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, Val Loss: {total_val_loss / len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f3021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab923c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = train_streamed.map(lambda x: tokenizer(x[\"text\"]))\n",
    "next(iter(tokenized_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\n",
    "next(iter(shuffled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_head = pubmed_dataset_streamed.take(5)\n",
    "list(dataset_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the first 1,000 examples and include the rest in the training set\n",
    "train_dataset = shuffled_dataset.skip(1000)\n",
    "# Take the first 1,000 examples for the validation set\n",
    "validation_dataset = shuffled_dataset.take(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90359b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_dataset_streamed = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    ")\n",
    "next(iter(law_dataset_streamed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce593681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac567a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b713d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071b763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e2c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b74830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
